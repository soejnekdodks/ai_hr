–û—Ç–ª–∏—á–Ω–æ! –£ —Ç–µ–±—è –µ—Å—Ç—å JSON —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—é–º–µ. –í–æ—Ç –∫–∞–∫ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö:

## üèóÔ∏è 1. –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–º–µ—á–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è NER

–ù—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:

```json
{
  "text": "–ö–∞–ª—É–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å –ú–æ—Å–∞–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω –≥ –ú–æ—Å–∞–ª—å—Å–∫ –ü–æ–¥—Å–æ–±–Ω—ã–π —Ä–∞–±–æ—á–∏–π –û–û–û –û–º–µ–≥–∞ —Å–±–æ—Ä—â–∏–∫ –æ–±—É–≤–∏",
  "annotations": [
    {"start": 0, "end": 15, "label": "LOCATION"},
    {"start": 16, "end": 32, "label": "LOCATION"},
    {"start": 33, "end": 44, "label": "LOCATION"},
    {"start": 45, "end": 61, "label": "POSITION"},
    {"start": 62, "end": 70, "label": "COMPANY"},
    {"start": 71, "end": 84, "label": "POSITION"}
  ]
}
```

## üìù 2. –°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

```python
import json
from typing import List, Dict, Any

def extract_ner_annotations(cv_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ"""
    
    text_parts = []
    annotations = []
    current_pos = 0
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π
    fields_to_extract = [
        ('localityName', 'LOCATION'),
        ('positionName', 'POSITION'),
        ('companyName', 'COMPANY'),
        ('jobTitle', 'POSITION'),
        ('instituteName', 'EDUCATION'),
        ('skills', 'SKILL'),
        ('languageKnowledge', 'LANGUAGE')
    ]
    
    for field, label in fields_to_extract:
        if field in cv_data and cv_data[field]:
            value = cv_data[field]
            
            if isinstance(value, str) and value.strip():
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
                text_parts.append(value)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                start = current_pos
                end = current_pos + len(value)
                annotations.append({
                    "start": start,
                    "end": end, 
                    "label": label
                })
                
                current_pos = end + 1  # +1 –¥–ª—è –ø—Ä–æ–±–µ–ª–∞
    
    # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    full_text = " ".join(text_parts)
    
    return {
        "text": full_text,
        "annotations": annotations
    }

def create_ner_dataset(cv_json_path: str, output_path: str) -> list[dict]:
    """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è NER –∏–∑ JSON —Ä–µ–∑—é–º–µ"""
    
    with open(cv_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    annotated_data = []
    
    for cv in data['cvs']:
        try:
            annotated = extract_ner_annotations(cv)
            if annotated['annotations']:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                annotated_data.append(annotated)
        except Exception as e:
            print(f"Error processing CV: {e}")
            continue
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(annotated_data, f, ensure_ascii=False, indent=2)
    
    print(f"Created dataset with {len(annotated_data)} examples")
    return annotated_data

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
create_ner_dataset('cv_data.json', 'ner_dataset.json')
```

## üè∑Ô∏è 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫–∏

```python
# config.py
LABELS = [
    "O",           # Outside
    "B-LOCATION", "I-LOCATION",     # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
    "B-POSITION", "I-POSITION",     # –î–æ–ª–∂–Ω–æ—Å—Ç—å
    "B-COMPANY", "I-COMPANY",       # –ö–æ–º–ø–∞–Ω–∏—è  
    "B-EDUCATION", "I-EDUCATION",   # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    "B-SKILL", "I-SKILL",           # –ù–∞–≤—ã–∫–∏
    "B-LANGUAGE", "I-LANGUAGE",     # –Ø–∑—ã–∫–∏
    "B-DATE", "I-DATE",             # –î–∞—Ç—ã
    "B-EXPERIENCE", "I-EXPERIENCE"  # –û–ø—ã—Ç
]
```

## üöÄ 4. –ü–æ–ª–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è

```python
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer,
    DataCollatorForTokenClassification
)
from datasets import Dataset
import numpy as np
from sklearn.model_selection import train_test_split

class NERTrainer:
    def __init__(self, config):
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])
        
        # –ú–µ—Ç–∫–∏
        self.labels = config['labels']
        self.id2label = {i: label for i, label in enumerate(self.labels)}
        self.label2id = {label: i for i, label in enumerate(self.labels)}
        
        # –ú–æ–¥–µ–ª—å
        self.model = AutoModelForTokenClassification.from_pretrained(
            config['model_name'],
            num_labels=len(self.labels),
            id2label=self.id2label,
            label2id=self.label2id
        )
    
    def load_dataset(self, dataset_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        with open(dataset_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def prepare_datasets(self, annotated_data):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        texts = [item['text'] for item in annotated_data]
        annotations = [item['annotations'] for item in annotated_data]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ IOB —Ñ–æ—Ä–º–∞—Ç
        tokenized_texts = []
        all_labels = []
        
        for text, anns in zip(texts, annotations):
            tokens = text.split()
            labels = ['O'] * len(tokens)
            
            # –†–∞–∑–º–µ—á–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            for ann in anns:
                entity_text = text[ann['start']:ann['end']]
                entity_tokens = entity_text.split()
                
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ tokens
                for i in range(len(tokens) - len(entity_tokens) + 1):
                    if tokens[i:i+len(entity_tokens)] == entity_tokens:
                        labels[i] = f"B-{ann['label']}"
                        for j in range(1, len(entity_tokens)):
                            labels[i+j] = f"I-{ann['label']}"
                        break
            
            tokenized_texts.append(tokens)
            all_labels.append([self.label2id[label] for label in labels])
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            tokenized_texts, all_labels, test_size=0.2, random_state=42
        )
        
        return train_texts, val_texts, train_labels, val_labels
    
    def __tokenize_and_align_labels(self, examples):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫"""
        tokenized_inputs = self.tokenizer(
            examples["tokens"],
            truncation=True,
            is_split_into_words=True,
            padding="max_length",
            max_length=128,
        )
        
        labels = []
        for i, label in enumerate(examples["ner_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            
            for word_idx in word_ids:
                if word_idx is None:
                    label_ids.append(-100)
                elif word_idx != previous_word_idx:
                    label_ids.append(label[word_idx])
                else:
                    label_ids.append(-100)
                previous_word_idx = word_idx
            
            labels.append(label_ids)
        
        tokenized_inputs["labels"] = labels
        return tokenized_inputs
    
    def __compute_metrics(self, eval_pred):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)
        
        # –£–±–∏—Ä–∞–µ–º padding —Ç–æ–∫–µ–Ω—ã
        true_predictions = [
            [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        precision = self._calculate_precision(true_predictions, true_labels)
        recall = self._calculate_recall(true_predictions, true_labels)
        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)
        
        return {"precision": precision, "recall": recall, "f1": f1}
    
    def train(self, train_data, val_data):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        # –°–æ–∑–¥–∞–µ–º datasets
        train_dataset = Dataset.from_dict({
            "tokens": train_data[0],
            "ner_tags": train_data[1]
        })
        val_dataset = Dataset.from_dict({
            "tokens": val_data[0], 
            "ner_tags": val_data[1]
        })
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        tokenized_train = train_dataset.map(
            self.__tokenize_and_align_labels,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        tokenized_val = val_dataset.map(
            self.__tokenize_and_align_labels,
            batched=True, 
            remove_columns=val_dataset.column_names
        )
        
        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=self.config['output_dir'],
            num_train_epochs=self.config['epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            learning_rate=self.config['learning_rate'],
            weight_decay=0.01,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
        )
        
        # –¢—Ä–µ–Ω–µ—Ä
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_val,
            tokenizer=self.tokenizer,
            data_collator=DataCollatorForTokenClassification(tokenizer=self.tokenizer),
            __compute_metrics=self.__compute_metrics,
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        trainer.train()
        trainer.save_model()
        
        return trainer

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
config = {
    'model_name': 'cointegrated/rubert-tiny2',
    'labels': LABELS,
    'output_dir': './ner_model',
    'epochs': 3,
    'batch_size': 16,
    'learning_rate': 2e-5
}

# –ó–∞–ø—É—Å–∫
trainer = NERTrainer(config)
annotated_data = trainer.load_dataset('ner_dataset.json')
train_data, val_data = trainer.prepare_datasets(annotated_data)
trainer.train(train_data, val_data)
```

