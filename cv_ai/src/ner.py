import json
import os
import re
from typing import Dict, List

import torch
from config import config
from peft import PeftModel
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoModelForTokenClassification,
    AutoTokenizer,
    pipeline,
)


def _build_json_instruction(mode: str) -> str:
    if mode == "resume":
        task = (
            "–ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. "
            "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –§–ò–û (PERSON), –∫–æ–Ω—Ç–∞–∫—Ç—ã (CONTACT, BIRTHDATE, LOCATION), "
            "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (UNIVERSITY, DEGREE, GRAD_YEAR, EDUCATION), "
            "–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã (COMPANY, POSITION, YEARS, EXPERIENCE, ACHIEVEMENT), "
            "–Ω–∞–≤—ã–∫–∏ (SKILL, TOOL, LANGUAGE, SOFT_SKILL)."
        )
    else:
        task = (
            "–ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ —É—Å–ª–æ–≤–∏—è. "
            "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (REQUIREMENT), –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ (RESPONSIBILITY), "
            "—É—Å–ª–æ–≤–∏—è (CONDITION), —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ (SKILL, TOOL, LANGUAGE), "
            "soft skills (SOFT_SKILL), –ª–æ–∫–∞—Ü–∏—é (LOCATION)."
        )
    return task


class NERModel:
    def __init__(self, model_path=None):
        self.id_to_label = {i: label for i, label in enumerate(config.LABELS)}
        self.label_to_id = {label: i for i, label in enumerate(config.LABELS)}

        if model_path and os.path.exists(model_path):
            self.__load_model(model_path)
        else:
            self.__initialize_model()

    def __initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.TOKEN_MODEL_NAME, add_prefix_space=True
        )

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model_config = AutoConfig.from_pretrained(
            config.TOKEN_MODEL_NAME,
            num_labels=len(config.LABELS),
            id2label=self.id_to_label,
            label2id=self.label_to_id,
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.model = AutoModelForTokenClassification.from_pretrained(
            config.TOKEN_MODEL_NAME, config=model_config
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ pipeline –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        self.pipeline = pipeline(
            "token-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            aggregation_strategy="simple",
        )

    def __load_model(self, model_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        model_config = AutoConfig.from_pretrained(
            model_path,
            num_labels=len(config.LABELS),
            id2label=self.id_to_label,
            label2id=self.label_to_id,
        )

        # üîë –¥–æ–±–∞–≤–ª–µ–Ω ignore_mismatched_sizes=True
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_path, config=model_config, ignore_mismatched_sizes=True
        )

        self.pipeline = pipeline(
            "token-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            aggregation_strategy="simple",
        )


class ResumeParser:
    def __init__(self):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + LoRA
        base = AutoModelForCausalLM.from_pretrained(
            config.BASE_MODEL, torch_dtype=torch.float16, device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)
        self.model = PeftModel.from_pretrained(base, config.LORA_MODEL)
        self.tokenizer = tokenizer
        self.model.eval()

        # –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–±–æ—Ä–∫–∞—Ö Qwen3 –Ω–µ—Ç eos_token_id ‚Äî –∑–∞—â–∏—Ç–∏–º—Å—è
        self.eos_token_id = getattr(self.tokenizer, "eos_token_id", None)

    def __build_prompt(self, text: str, mode: str = "resume") -> str:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–æ–¥–µ–ª–∏ ChatML-—Ñ–æ—Ä–º–∞—Ç
        # –∏ —è–≤–Ω–æ –∑–∞–ø—Ä–µ—â–∞–µ–º —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è / chain-of-thought.
        instruction = _build_json_instruction(mode)
        return (
            f"<|im_start|>system{config.SYSTEM_PROMPT}<|im_end|>"
            f"<|im_start|>user{instruction}–¢–µ–∫—Å—Ç:{text}<|im_end|>"
            f"<|im_start|>assistant"
        )

    @staticmethod
    def _json_only(s: str) -> Dict[str, List[str]]:
        def _default():
            return {k: [] for k in config.LABELS}

        try:
            obj = json.loads(s)
        except Exception:
            # fallback ‚Äî –∏—â–µ–º –ø–µ—Ä–≤—ã–π –±–æ–ª—å—à–æ–π –±–ª–æ–∫ {...}
            matches = re.findall(r"\{.*\}", s, flags=re.DOTALL)
            if not matches:
                return _default()
            for frag in matches:
                try:
                    obj = json.loads(frag)
                    break
                except Exception:
                    continue
            else:
                return _default()

        result = {}
        for k in config.LABELS:
            val = obj.get(k, [])
            if isinstance(val, str):
                result[k] = [val]
            elif isinstance(val, list):
                result[k] = [str(x) for x in val if isinstance(x, (str, int, float))]
            else:
                result[k] = []
        return result

    @staticmethod
    def _normalize(items: List[str]) -> List[str]:
        out = []
        for x in items:
            x = str(x).strip().lower()
            x = re.sub(r"[^a-z–∞-—è—ë0-9@+/#.\- ,;:()]+", "", x, flags=re.IGNORECASE)
            x = re.sub(r"\s+", " ", x)
            if len(x) > 1:
                out.append(x)
        seen, uniq = set(), []
        for x in out:
            if x not in seen:
                uniq.append(x)
                seen.add(x)
        return uniq

    def _run_model(self, prompt: str) -> str:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç (–±–µ–∑ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏)."""
        inputs = self.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self.model.device)

        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=384,
                do_sample=False,  # –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
                temperature=0.0,
                eos_token_id=self.eos_token_id,
            )

        raw = self.tokenizer.decode(
            out[0][inputs.input_ids.shape[-1] :], skip_special_tokens=True
        )
        return raw

    def extract_entities(self, text: str, mode: str = "resume") -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∫–ª—é—á–∞–º–∏ –∏–∑ config.LABELS.
        –î–≤—É—Ö—à–∞–≥–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥:
        1. –ü—Ä–æ–≥—Ä–µ–≤ ‚Äî –º–æ–¥–µ–ª—å –¥—É–º–∞–µ—Ç, –Ω–æ –≤—ã–≤–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ 'OK'.
        2. –û—Å–Ω–æ–≤–Ω–æ–π —à–∞–≥ ‚Äî –º–æ–¥–µ–ª—å –≤—ã–¥–∞—ë—Ç —á–∏—Å—Ç—ã–π JSON.
        """

        # 1. –ü—Ä–æ–≥—Ä–µ–≤
        warmup_prompt = (
            f"<|im_start|>system{config.SYSTEM_PROMPT}<|im_end|>"
            f"<|im_start|>–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –ø–æ–¥—É–º–∞–π, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å. "
            f"–í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ 'OK'. –¢–µ–∫—Å—Ç: {text}<|im_end|>"
            f"<|im_start|>assistant"
        )
        _ = self._run_model(warmup_prompt)

        # 2. –û—Å–Ω–æ–≤–Ω–æ–π –≤—ã–∑–æ–≤
        prompt = self.__build_prompt(text, mode)
        raw = self._run_model(prompt)

        raw = raw.lstrip(" :\n\t")
        raw = raw.replace("```json", "").replace("```", "")
        raw = re.sub(r"<think>.*?</think>", "", raw, flags=re.DOTALL).strip()

        print(raw)

        data = self._json_only(raw)
        return {k: self._normalize(v) for k, v in data.items()}
