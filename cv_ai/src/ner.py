import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from config import config
from peft import PeftModel
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoModelForTokenClassification,
    AutoTokenizer,
    pipeline,
)



<<<<<<< HEAD

class NERModel:
    def __init__(self, model_path=None):
        self.id_to_label = {i: label for i, label in enumerate(config.LABELS)}
        self.label_to_id = {label: i for i, label in enumerate(config.LABELS)}

        if model_path and os.path.exists(model_path):
            self.__load_model(model_path)
        else:
            self.__initialize_model()

    def __initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.TOKEN_MODEL_NAME, add_prefix_space=True
        )

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model_config = AutoConfig.from_pretrained(
            config.TOKEN_MODEL_NAME,
            num_labels=len(config.LABELS),
            id2label=self.id_to_label,
            label2id=self.label_to_id,
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.model = AutoModelForTokenClassification.from_pretrained(
            config.TOKEN_MODEL_NAME, config=model_config
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ pipeline –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        self.pipeline = pipeline(
            "token-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            aggregation_strategy="simple",
        )

    def __load_model(self, model_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        model_config = AutoConfig.from_pretrained(
            model_path,
            num_labels=len(config.LABELS),
            id2label=self.id_to_label,
            label2id=self.label_to_id,
        )

        # üîë –¥–æ–±–∞–≤–ª–µ–Ω ignore_mismatched_sizes=True
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_path, config=model_config, ignore_mismatched_sizes=True
        )

        self.pipeline = pipeline(
            "token-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            aggregation_strategy="simple",
        )

class ResumeParser:
    def __init__(self):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + LoRA
        base = AutoModelForCausalLM.from_pretrained(
            config.BASE_MODEL, torch_dtype=torch.float16, device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)
        self.model = PeftModel.from_pretrained(base, config.LORA_MODEL)
        self.tokenizer = tokenizer
        self.model.eval()

        # –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–±–æ—Ä–∫–∞—Ö Qwen3 –Ω–µ—Ç eos_token_id ‚Äî –∑–∞—â–∏—Ç–∏–º—Å—è
        self.eos_token_id = getattr(self.tokenizer, "eos_token_id", None)

    def __build_prompt(self, text: str, mode: str = "resume") -> str:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–æ–¥–µ–ª–∏ ChatML-—Ñ–æ—Ä–º–∞—Ç
        # –∏ —è–≤–Ω–æ –∑–∞–ø—Ä–µ—â–∞–µ–º —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è / chain-of-thought.
        instruction = _build_json_instruction(mode)
        return (
            f"<|im_start|>system{config.SYSTEM_PROMPT}<|im_end|>"
            f"<|im_start|>user{instruction}–¢–µ–∫—Å—Ç:{text}<|im_end|>"
            f"<|im_start|>assistant"
=======
class ResumeVacancyAnalyze:
    def __init__(self, model_name: str = None, device_map="auto"):
        self.model_name = model_name or config.BASE_MODEL

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map=device_map,
            trust_remote_code=True,
>>>>>>> app_branch
        )

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )

        if self.tokenizer.pad_token_id is None:
            if self.tokenizer.eos_token_id is not None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.add_special_tokens({"pad_token": "[PAD]"})
                self.model.resize_token_embeddings(len(self.tokenizer))

<<<<<<< HEAD
        result = {}
        for k in config.LABELS:
            val = obj.get(k, [])
            if isinstance(val, str):
                result[k] = [val]
            elif isinstance(val, list):
                result[k] = [str(x) for x in val if isinstance(x, (str, int, float))]
            else:
                result[k] = []
        return result

    @staticmethod
    def _normalize(items: List[str]) -> List[str]:
        out = []
        for x in items:
            x = str(x).strip().lower()
            x = re.sub(r"[^a-z–∞-—è—ë0-9@+/#.\- ,;:()]+", "", x, flags=re.IGNORECASE)
            x = re.sub(r"\s+", " ", x)
            if len(x) > 1:
                out.append(x)
        seen, uniq = set(), []
        for x in out:
            if x not in seen:
                uniq.append(x)
                seen.add(x)
        return uniq

    def _run_model(self, prompt: str) -> str:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç (–±–µ–∑ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏)."""
        inputs = self.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self.model.device)
=======
    def _run_model(self, prompt: str, max_new_tokens: int = 128) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True)
        device = next(self.model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
>>>>>>> app_branch

        with torch.no_grad():
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ generate
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é
                temperature=0.0,
                top_p=0.95,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        text = self.tokenizer.decode(out[0], skip_special_tokens=True)
        return text

<<<<<<< HEAD
        raw = self.tokenizer.decode(
            out[0][inputs.input_ids.shape[-1] :], skip_special_tokens=True
=======
    def score_resume_vs_job(self, resume_text: str, vacancy_text: str) -> float:
        prompt = (
            "No eplanation and extra text!"
            "Vacancy:\n"
            f"{vacancy_text}\n\n"
            "Resume:\n"
            f"{resume_text}\n\n"
            "You are an strict hr assistant that compares a candidate resume and a job vacancy. "
            "Return a single number between 0 and 100 ‚Äî the percent match (no explanation).\n"
            "Answer with a single number only."
>>>>>>> app_branch
        )

        raw_output = self._run_model(prompt) # –ü—Ä–æ–≥–Ω–∞–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–æ–º–ø—Ç–µ
        raw_output = self._run_model(prompt) # –û–Ω–∞ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç

        numbers = re.findall(r"\d+(?:[\.,]\d+)?", raw_output)
        if numbers:
            try:
                num = float(numbers[0].replace(',', '.'))
                return min(max(num, 0.0), 100.0)
            except ValueError:
                pass

        return 0.0