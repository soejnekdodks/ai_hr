from sentence_transformers import SentenceTransformer, util
import sys
import os

# Add the project root directory to Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from cv_ai.src.ner import NERModel

class EntityMatcher:
    def __init__(self):
        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    def vectorize_entities(self, entities):
        """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        entity_texts = []
        entity_weights = []
        
        for entity in entities:
            entity_texts.append(entity['word'])
            entity_weights.append(entity['score'])  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫–∞–∫ –≤–µ—Å
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        embeddings = self.embedder.encode(entity_texts)
        
        return embeddings, entity_weights
    
    def calculate_similarity(self, embeddings1, embeddings2, weights1, weights2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        similarity_matrix = util.cos_sim(embeddings1, embeddings2)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
        total_similarity = 0
        total_weight = 0
        
        for i in range(len(embeddings1)):
            for j in range(len(embeddings2)):
                similarity = similarity_matrix[i][j].item()
                weight = weights1[i] * weights2[j]  # –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ—Å–æ–≤
                total_similarity += similarity * weight
                total_weight += weight
        
        return total_similarity / total_weight if total_weight > 0 else 0


    def compare_by_category(self, resume_entities, vacancy_entities):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        category_scores = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        resume_by_category = {}
        vacancy_by_category = {}
        
        for entity in resume_entities:
            category = entity['entity_group']
            if category not in resume_by_category:
                resume_by_category[category] = []
            resume_by_category[category].append(entity)
        
        for entity in vacancy_entities:
            category = entity['entity_group']
            if category not in vacancy_by_category:
                vacancy_by_category[category] = []
            vacancy_by_category[category].append(entity)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        for category in set(resume_by_category.keys()) | set(vacancy_by_category.keys()):
            resume_cat = resume_by_category.get(category, [])
            vacancy_cat = vacancy_by_category.get(category, [])
            
            if resume_cat and vacancy_cat:
                # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                resume_emb, resume_weights = self.vectorize_entities(resume_cat)
                vacancy_emb, vacancy_weights = self.vectorize_entities(vacancy_cat)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self.calculate_similarity(
                    resume_emb, vacancy_emb, resume_weights, vacancy_weights
                )
                category_scores[category] = similarity
        
        return category_scores

class ResumeVacancyMatcher:
    def __init__(self, model):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º NER –º–æ–¥–µ–ª—å
        self.ner_pipeline = model.pipeline

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.category_weights = {
            "POSITION": 0.4,    # –î–æ–ª–∂–Ω–æ—Å—Ç—å - —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π
            "SKILL": 0.3,       # –ù–∞–≤—ã–∫–∏
            "LOCATION": 0.15,   # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            "COMPANY": 0.05,    # –ö–æ–º–ø–∞–Ω–∏—è
            "EDUCATION": 0.05,  # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            "EXPERIENCE": 0.05  # –û–ø—ã—Ç
        }
    
    def match(self, resume_text, vacancy_text):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏—é"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        resume_entities = self.extract_entities(resume_text)
        vacancy_entities = self.extract_entities(vacancy_text)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_scores = self.compare_by_category(resume_entities, vacancy_entities)
        
        # –û–±—â–∏–π score (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞)
        total_score = 0
        for category, score in category_scores.items():
            weight = self.category_weights.get(category, 0.01)
            total_score += score * weight
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 0-100
        final_score = min(100, max(0, total_score * 100))
        
        return {
            "score": round(final_score, 1),
            "category_scores": category_scores,
            "resume_entities": resume_entities,
            "vacancy_entities": vacancy_entities
        }
    
    def extract_entities(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        return self.ner_pipeline(text)
    
    def compare_by_category(self, resume_entities, vacancy_entities):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        category_scores = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        resume_by_cat = self._group_by_category(resume_entities)
        vacancy_by_cat = self._group_by_category(vacancy_entities)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        all_categories = set(resume_by_cat.keys()) | set(vacancy_by_cat.keys())
        
        for category in all_categories:
            resume_items = resume_by_cat.get(category, [])
            vacancy_items = vacancy_by_cat.get(category, [])
            
            if resume_items and vacancy_items:
                # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º
                resume_emb = self.embedder.encode([e['word'] for e in resume_items])
                vacancy_emb = self.embedder.encode([e['word'] for e in vacancy_items])
                
                # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = util.cos_sim(resume_emb, vacancy_emb).mean().item()
                category_scores[category] = similarity
            else:
                category_scores[category] = 0.0
        
        return category_scores
    
    def _group_by_category(self, entities):
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        grouped = {}
        for entity in entities:
            category = entity['entity_group']
            if category not in grouped:
                grouped[category] = []
            grouped[category].append(entity)
        return grouped

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
model = NERModel()

matcher = ResumeVacancyMatcher(model = model)

# –†–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏—è
resume_text = """
–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ —Å 5 –ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.
–†–∞–±–æ—Ç–∞–ª –≤ –Ø–Ω–¥–µ–∫—Å–µ —Å Django –∏ Flask. 
–ü—Ä–æ–∂–∏–≤–∞–µ—Ç –≤ –ú–æ—Å–∫–≤–µ. –í—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ú–ì–£.
"""

vacancy_text = """
–ò—â–µ–º Senior Python developer —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã –æ—Ç 3 –ª–µ—Ç.
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: Django, Flask, PostgreSQL. 
–†–∞–±–æ—Ç–∞ –≤ –ú–æ—Å–∫–≤–µ. –û—Ñ–∏—Å –≤ —Ü–µ–Ω—Ç—Ä–µ –≥–æ—Ä–æ–¥–∞.
"""

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
result = matcher.match(resume_text, vacancy_text)
def print_match_details(result):
    """–ö—Ä–∞—Å–∏–≤–∞—è –ø–µ—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print("üéØ Match Results")
    print(f"Overall Score: {result['score']}%")
    print("\nüìä Category Scores:")
    for category, score in result['category_scores'].items():
        print(f"  {category:12}: {score:.3f}")
    
    print("\nüìù Resume Entities:")
    for entity in result['resume_entities']:
        print(f"  {entity['word']:15} ‚Üí {entity['entity_group']}")
    
    print("\nüìã Vacancy Entities:")
    for entity in result['vacancy_entities']:
        print(f"  {entity['word']:15} ‚Üí {entity['entity_group']}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
print_match_details(result)



# import os 
# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
# from transformers import AutoConfig
# from src.config import config

# class CVParserPipeline:
#     def __init__(self, config_path):
#         self.config = self.load_config(config_path)
#         self.initialize_components()
    
#     def initialize_components(self):
#         """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
#         self.data_loader = DataStreamer(
#             self.config['data_path'],
#             self.config['batch_size']
#         )
#         self.preprocessor = TextPreprocessor()
#         self.ner_model = NERModel(self.config['model_path'])
#         self.postprocessor = ResultPostprocessor()
#         self.cache = RedisCache() if self.config['use_cache'] else SimpleCache()
    
#     async def process_stream(self):
#         """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
#         async for batch in self.data_loader.async_stream():
#             processed_batch = await self.process_batch(batch)
#             yield processed_batch
    
#     async def process_batch(self, batch):
#         """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö"""
#         results = []
        
#         for cv_data in batch:
#             # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
#             normalized = self.preprocessor.normalize_cv(cv_data)
#             text = self.extract_text_for_ner(normalized)
            
#             # –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
#             cache_key = self.generate_cache_key(text)
#             if cached := self.cache.get(cache_key):
#                 results.append(cached)
#                 continue
            
#             # NER –æ–±—Ä–∞–±–æ—Ç–∫–∞
#             entities = await self.ner_model.predict_async(text)
#             structured = self.postprocessor.structure_entities(entities, text)
            
#             # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à
#             self.cache.set(cache_key, structured)
#             results.append(structured)
        
#         return results
    
#     def run(self):
#         """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
#         for batch in self.data_loader.stream_jsonl():
#             results = self.process_batch(batch)
#             self.save_results(results)
#             self.monitor.progress_update(len(results))


# from src.ner import NERModel
# from src.config import config

# def cv_analise(text: list[str]) -> None:
#     model = NERModel()
#     dct_out = {tag: "" for tag in config.LABELS}
#     for line in text:
#         results = model.pipeline(line)
#         for entity in results:
#             # TODO –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É, —á—Ç–æ–±—ã –±—ã–ª —Å–ª–æ–≤–∞—Ä—å: –∫–ª—é—á–∏-—Ç–µ–≥–∏, –∑–Ω–∞—á–µ–Ω–∏—è-—Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤
#             pass

